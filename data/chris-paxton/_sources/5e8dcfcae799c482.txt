Hey everyone, welcome to another episode of Robbo Papers. I'm Chris and I'm here with Arhan and Carl and uh yeah, would you guys like to introduce yourselves and get started? >> I guess I can start. I'm Arhan. I'm a first year PhD student at the University of Washington. I'm advised by Abhishek Gupta and I guess what I'm going to talk about is like my the first project I did in my PhD with Carl and Abishek and many other collaborators on using sim for robot evaluation. >> Yeah, my name is Carl. I'm a member of technical staff at physical intelligence and I like to work on large scale robot learning problems and had the pleasure of helping Arhan a little bit with his project here. >> Right. Um I guess I can give like a highle overview to start um of like what it is. So I I guess I briefly touched on like using sim for evaluation, but I think more importantly what we wanted to do was a scalable way maybe I should go here first, a scalable way to do um sim evaluation. Um and there's been like previous works like simpler and robot arena and few others. Um, but I think like a few key things that were missing previously were they're either too hard to construct without like um handcrafting your assets which a lot of like I guess robot learning people don't necessarily have access to um along with some of them don't really support wrist cameras uh such as simpler and like this robot arena work um have this limitation where if you're doing like green screening on your ex from your external camera views you can't really have a camera that moves around in your environment. environment. Um, and so one thing that we're doing differently like approach we took was using like a real to sim approach to generate environments to make it cheap to get eval environments. Um, along with like if you have some sort of explicit representation like a gausian slot or something, you can actually move around in your environment and um get novel views that are not necessarily like just green screens of your uh training data set. Um, and I guess >> yeah, >> I think it may be helpful to actually zoom out a little bit first and try to kind of explain why should we even do this kind of evaluation for robotics at all? >> Sure. >> Uh, okay. Do you want me to do like a one minute summary of that? >> Uh, yeah, sure. Go for it. Yeah. >> Okay. So, so basically I think you know if you thought of it from from the start you would say okay as a roboticist we should always do real world evolves, right? because real world evolves are what we care about. This is kind of the metric that we're optimizing for is to have robots actually work in the real world. But then when you do that in practice and I'm sure many of the people who are listening and who are roboticists know the pain of that. Um it's not actually very scalable because running real robot evolves is really painful and it takes a lot of time and it's very hard to reproduce. Um and so even historically as a community we have used a lot of sim eva right like we have often built benchmarks. I think Jeff also has some great benchmarks uh that that he built in simulation. Um and the benefit is clear, right? Like it's very fast, it's very scalable, it's perfectly reproducible. Um so so everybody loves SIM evals. Um but then that there's kind of this disconnect, right? Like we care about real world performance, but then we build SIM evals and we all test on SIM evals. Um, and so I think this work here is kind of trying to close this gap a little bit and try to build evals that are SIM evals, but they're actually not really trying to optimize for like the best performance in SIM, but what instead they're trying to optimize for is to be indicative of real world performance. Um, so so the policies that we will evaluate in this paper today are actually trained on real data most of the time. um and they can run on real robots and in fact a lot of the experiments that Ran did for this paper are real robot experiments. Um but then the final product product so to say or the outcome of this paper is a sim eval tool where you run a simulated evaluation like any other sim eval. But then what we show for example in this little graph on the right here is that the performance of these policies in the simulation are actually indicative of what they will do in real like basically a policy that does better in our sim benchmark will do better in real right and so this is where the value is basically we can build a simulated tool that allows you to test performance that then hopefully is similarly reflected in the real world. Okay. And so I think now is maybe a good time to then zoom in and say okay what was there before because we're not the first people to work on sim eval. Uh and then you know how does our technical solution expand the capabilities of what was there before. So I'll just hand it back to Arhan to >> stand up. >> Yeah. >> Um yeah, >> just to like add a bit on what K mentioned like if it would be great if like uh if Kan could give a bit of context to some of our non-rootics audience like you know why is evaluation so much harder for robotics as compared to >> LMS VMs you know computer vision NLP. Yeah I think that would be that that would be really great for the audience. Yeah >> yeah that's a good point. I think like so I think one thing that makes robotics particularly hard compared to all these other problems that like um fields that you mentioned is uh the issue of compounding error in our case like most of these other like offline metrics uh like validation MSSE I guess okay I I should say compounding error and a multimodality uh I think like all these other metrics that they use like validation MSSE and like um I guess they're mostly like onstep metrics where like if you're predicting the next uh like if you're predicting like an answer to uh what's in this image or um if you're like predicting uh like housing prices or something like these are like onestep problems and in robotics like whatever you predict affects your what you see next and so you kind of have this issue of compounding error where you like you will start to drift away from like the more errors in prediction you'll get larger drift and so your your evaluation metric might not truly represent what your policy can do. Um I guess so this is like the issue between like doing open loop versus closed loop evals and why open loop is hard. Um and so that's why like we we're trying to build like use simulators to actually be able to roll out your policy effectively. Um yeah, >> it's actually quite an interesting point because I don't actually think that like for example LLMs will get around this problem for much longer. Like in some sense, you know, LLM can choose to start with problems that are singlestep problems where you just like take an input and produce a output and that's it. But over time kind of the problems that the LM want to tackle become more sequential decision-m problems, right? like they may want to interact with users or they may want to do very complicated things in code bases where maybe they need to run code and wait for a result and do it again and so on and so the sequential nature will come into LM2 and then they will have a lot of these same problems where they maybe need to simulate users now because that's their environment um and then and then again they will have similar problems where evaluation becomes hard um but so far they could kind of choose to not do sequential decision-m and so they had easier evalu for robotics you cannot make that choice because there's no use for robotics without sequential decision- making and so we have to tackle this problem from the start >> yeah sounds great yeah please continue >> yeah yeah yeah cool I guess like next I so I can get into um what we actually did like what our method was so I I talked a little bit about like we were doing this sort of like 3D reconstruction to make it cheap to create new evaluation environments um specifically what we were doing is like using uh 2D gausian splatting for like extracting a mesh and then you get have gausian splatting for your visuals. Um and then we have this like scene composition thing that I can cover a little bit later maybe. Um where you can like basically pull in different assets that you scan. Um for environments we're using like gausian splatting but for like objects we're doing uh more generative models like uh in the paper we use trellis but recently more things have come out like uh sand 3D for example which just you give like single view or multiv- view inputs and then you can get a generative uh model to produce the object. Uh like the benefit of this why we use this for objects versus actually scanning it like normally um is because your objects are going to be partially observable. Like if it's sitting on a table, you won't really be able to capture what the bottom of the surface looks like. So that's why generative models are helpful here cuz it can do like the completion what what you can't see. Um, but anyways, I guess you put this together in the scene builder and then you can kind of get a scene with like a task that you want um that you care about uh that is paired with the real world. Um and in practice what we do uh we tried to get this to work zero shot in the in the beginning and I can actually uh show the plot like right after this but um zeroot if you transfer your like real like policies that are only trained on real world data if you transfer them to this simulation um that's like g like with using gausian splatting background visuals and like um either rate traced or splatted objects as well we actually see like your policy will do something reasonable and like if you have don't do like systematic uh analysis it'll look like like the policy is transferring to sim like this is probably a good evaluation but if you do like the paired world paired real world correlations between them the zeroot transfer is not super highly correlated and what we found is like if you throw in some code like you throw in a short like code training uh data set that's not task specific or environment specific to what you're testing on just like random simulation data into your code training mix and then evaluate your policy it the correlations look much much better. Um and the intuition behind this is just like you're doing some sort of alignment uh to get around the distribution shift your policy sees from going to from real to sim. Um and so basically on the right you have this like example of what some of our correlations look like after doing some short sim code training. Um, and I want to stress like the data that we pick for co-raining is like completely irrelevant to what we test on. Like there's no overlap um in our evaluation environments where we >> It's not even like the same robot or anything like that. >> Oh, okay. It is it is the same robot. I will say that. Um, it'd be interesting to see like how it like I guess it was not really in our like in the academic capacity to test it on like multi-mbbodyments if you just throw in random sim data. Um I I actually I don't know how how that would change things, but we did the we use the same like droid setup, but like all our objects and environments completely different. >> So you to be fair, the policies >> the policies we're evaluating here are basically single robot policies, right? They were at some point pre-trained on multi-root data sets, but then for all practical purposes, they're really just heavily fine-tuned on the single robot embodiment of a Franka robot. And so it probably makes most sense to put some data for that same robot in there. >> Can I ask how you're thinking about physics in all this? Oh, okay. Well, let me start. Yeah. So, because uh if you're doing gosh and splatting, obviously there's no like you don't have any idea of like what the frictions are or the masses of the different objects and presumably maybe your code is giving you some of that for the robot, but like so could you tell me how you think about this? >> So like uh to clarify is a question on like how we're specifying like >> Yeah. like do you randomize it or what do you how do you how how do you like or is this not an important problem or like what is how do you think about it that's a very open-ended I guess >> yeah I think um so so most of the things that we evaluated were like rigid body tasks uh where it's like the objects that we place in front of them like most of them are the robot should be able to pick them up and I think there could be like stronger CIS ID done we we didn't do like extensive CIS ID basically cuz what we were testing testing. What we tested was like we did test at some point is like dynamics between SIM and real different. Um we basically did this like hardware in the loop test uh as we called it but basically what it was was we roll out the policy in the real world but the inputs to the like policy is not real world inputs it's just like mirrored SIM inputs. Um sorry wait other way around. It's you roll out the policy in SIM but the inputs to the policy are real world inputs. So basically you isolate the fact that like you get real world visuals but you isolate the sim dynamics um and we that was basically a test to see like our dynamics and issue whether it's like for object masses masses and friction or like robot dynamics too um like controllers and stuff >> but that would that would mostly only work on tele contact right or >> uh so for the the robot stuff it would mostly work on contact I guess most of the objects that we're doing like if it's able to pick it up, it will pick it up. Um, and for that reason, we didn't focus too hard on like object mass and friction randomization. Um, I think when you get into more like dextrous tasks and maybe like deformables, this will definitely become a much bigger issue. Um, in our case, we basically just set them to like kind of like default values approximated by like this the size of the object and like the density or whatever. >> Um, it was like auto computally. Yeah. >> Like I think these are kind of observations that are somewhat in line with what we had seen in previous work as well. Uh so in the simpler paper which is kind of a uh previous version of a similar at least problem setting if very different approach um there we explicitly try to see how sensitive the whole system is to different physics parameters and it wasn't particularly sensitive um and so Arhan's experiment here is actually a very nice way to delineate these two components of a simulated evaluation system which are the dynamics and the visuals and I think it was a quite nice demonstration that actually the dynamics at least in these times tasks as Arhan said are not kind of the key driver of correlation issues but it's really mostly about the visuals um which is actually what in the first place motivated us to use the gausian spliding approach because it's one way to get very high quality visuals with relatively low effort on the user side. Um like basically making a very very nice looking gausian plat is much much easier than making a very very nice looking handcrafted simulation environment. Um and so the ability to get these nice visuals quickly was kind of one of the key motivations to use this technique. It also did turn out that the visuals are really what is driving good correlation. >> I have a I have a question just regarding one of the points that was made which is the uh leveraging sim data into the code training. So I guess you did like code training with simroid data that's collected on different tasks and that was like about 10% of the data set. No, it's Oh. Oh, I see. It's maybe 10% in the code training mixture, but it is a very very tiny data set compared to the actual droid training data set. >> Yeah. So, that's the question. >> Why would the correlation get better if the amount of sim data is so little? >> Yeah. So, so I think there's an important kind of insight here, which is that the sim data is not there to teach the robot something it doesn't already know. >> Um, it's there to teach the robot how to ignore certain differences that are spirious correlations. Um, right. So, so in in your sim data, you have certain artifacts that are induced by say the rendering of a gausian splat which looks a little fuzzy for example when you get close like wrist cameras get really close um and you don't have that on your real data but all the robot has seen is real data so far right so this is kind of confusing and and these models tend to not generalize like humans they tend to have particular um kind of properties where if you have a small distribution shift like this it can really throw off a model um but clearly it's nothing about the task right like the tasks we have here are very similar to what we have seen in real. It's just that the visuals look a little bit different >> in a way that's unintuitive for humans but really makes a difference for these models. >> And so the purpose of the sim data is only to kind of teach the model that actually you can be robust to this difference like whether the image looks like a real image or a slightly washed version of a real image doesn't actually matter for what you should do in this environment. Um, and so that's why just like a tiny bit of sim data on unrelated tasks and only train on it for a few hundred steps is enough >> um to essentially teach the model this robustness and then your correlation results get much much better. >> Yeah, I'm convinced that you know it doesn't help with the task but really just to get the correlation better. I'm just curious like do you guys try different mixture to see whether does the correlation gets better and better as your sim increases or is it you know like is there a a correlation to actually scaling sim to so that the co relation between real and sim gets more and more aligned uh that yeah >> so I think we had so we couldn't scale it too far just given how hard it is like to collect like data in the first place Um, but we had I guess we had like two phases of like or two amounts of sim that we tested on which is like not like many like a lot of data points. I'd be curious to see how it changes as you scale more sim data. Um, but between like I think the first one was on the order of like I'd say like six environments and like this one was closer to 20 uh 15 to 20. Um, and the correlations did improve. Uh, but I guess it just becomes more expensive to find more out of distribution environments and tellyop demonstrations to put into sin data after that. Um, yeah. What is it? >> Yeah, I think there was actually a very interesting kind of nuanced point in that experiment. Aran, I don't know if we have the plot somewhere in the paper maybe um about the different types of sim code training data that we tested. >> Oh, yeah. Um because there's a few different choices you could make, right? You could actually collect data in totally different environments. You could collect data in the same environment but on different tasks. And you could collect data literally on the task that you're planning to eval on, right? like these are all reasonable choices. Um and it turns out that they have different effects on the correlation values. Um and so specifically uh I guess if you try to com uh let's see I think in domain is basically what we called the collecting data on the exact task that you're going to eval and maybe naively you would think that's actually the best thing to do right you would think oh the closer the data is to what I want to eval the better the model should be so that's good but actually this is not the optimal thing to do right because in some sense what you're going to get then is a model that overfits to this like small sim data set that essentially tells it the solution to your test that right like you're you're going to eval on those same tasks. Um and that makes all of your policies better, but it doesn't make your evaluation discriminative any longer, right? Like basically now your all your policies kind of do well roughly speaking. Um whereas if you have data that's only vaguely related and tells your model how to bridge this distribution shift but doesn't kind of give it the answer to the test that you're going to pose, um then you get better correlations in the end, right? So so I think there's a bit of a nuance point. How do you want to choose this data set? you actually don't want it to be too close uh to your test set because then your test set loses kind of some of its uh discriminability between different policies. >> Yeah, I think this is what the plot basically shows. I think the the yeah the the purple plot here is is what our final method ends up doing um which is to just train on data that is not featured in the environment that you're testing on. Training on other tasks in the same environment is also a good choice. It's actually in some sense a little bit of a better choice. Um but then training on tasks that you're literally going to evaluate on is somewhat worse uh than than >> and I think like one of the nice things is like having these like unrelated tasks in your code training data set makes it much more easy to just evaluate a new task rather than to have to collect data in your new environment that you want to evaluate or something like that. Okay. Uh was there any other question? I think maybe another interesting maybe if you want to go to the main result figure um I feel another interesting kind of higher level point uh that we evaluated is that there's a lot of people who are quite excited about using video models. Um is it on this plot? Maybe you need to go onto the paper. >> It'll be more clear on this one. >> Yeah. >> Um so so basically there's a lot of people who are excited about using video models to evaluate policies. uh which is an even easier way in some sense to get an evaluation environment because the model just is the environment. Um and so for some context what this would mean is that you still have your policy it's trained on real data you have a video model that's trained on real data uh and then you just kind of loop them. So you have your policy look at the outputs of the video model, produce the next action, you pass that into your video model at this one step or you may want to call the world model if it's action conditioned. Uh and then you loop, right? And so you basically get a roll out of your policy in the video model. You can score it at the moment people do this by hand. Um and then you can see whether the performance in that video model is indicative of real world performance, right? And so again an important part here is we don't just care that policies do something reasonable. we actually want these evaluations to be correlated with our real world results, right? And and so we did actually quite a rigorous test of this in in this project where we tried um the current best open-source um droid model um that is available. Uh and we ran our policies through that model and we did find that you know they do something for sure. they're not just like randomly waving the arm or something, but the actual correlation if it comes down to it is not nearly as strong as what you get if you use a more classic way of simulating um the environment at the moment. Right? There's in some sense a bit of a continuum like we have the fully handcrafted sim. We have the fully learned world model simulator. Um and the H project is somewhere in the middle, right? Where you have like a bit of learn structure and visuals, but then the actual physics are still classic. And so at the moment that seems to kind of give you the best trade-off of how easy it is to make versus how good the correlation is you end up with in the end. >> Can you talk a little bit more about how you set up the video model and why this would be fail why this would do worse? So is this is this basically just because of um predictions getting worse over time or what why why is this like is it just diverging or what's the problem? >> I think I think there's two kind of parts of it. So there's there's one is that like divergence over time. I think the first one I'll touch on is like um one a lot of it is yeah I'll talk about the divergence first like the divergence like in some cases it even becomes like hard to grade where like these like objects are not necessarily >> objects vanish or something is that the >> yeah they vanish but they like come back to where they started from >> u so that's one part of it uh I guess that's not the only thing like messing up correlations that that mainly just makes it hard to grade sometimes. Uh the other thing is like I think a lot of the trajectories you will probably see are going to be success trajectories. Um and so like if your world model has like if it's I guess I I don't know the exact data mix of how this this world model specifically was trained, but you there's like so many ways to fail compared to how many there are that you will succeed. So being able to just like model that from like being able to collect that amount of like su failure data is going to be hard. So just at least in like the data that they're being trained in right now, it's more likely that you will gravitate towards success. Like that's kind of the intuition. So even if like the action was like if the action was like 2% off and that would have cost a failure, it's just the point is that that's not going to be well represented in the underlying video data set and then it's going to fail. Is that the is that the intuition? >> Yeah, I think so. And then the other thing is like I guess like you can work there's there's probably works going on right now honestly on like trying to make them more action conditionable and like true to like what the outcome is. But also I guess if you have it's it's kind of hard to know like what the deviation is other than like doing some sort of like image similarity score too. So that makes it hard to like >> hard to assess. Yeah. >> Yeah. Like I think you know I I don't >> Yeah. Like I think you know I I don't think the outcome that people should think the outcome that people should take away from this discussion is oh take away from this discussion is oh video model evolves will never work or video model evolves will never work or something like this or like you know something like this or like you know this is the final solution. Yeah. this is the final solution. Now, um I I [clears throat] think the upper ceiling of video model evals is much higher, right? Like if you think about what kind of tasks can you actually test um in a classic simulator like the one that we have here, it's great for rigid objects. It's probably decent for articulate objects, it's going to be a whole lot harder to like, I don't know, simulate a cloth folding task um or simulate a wiping task with some liquid in there, right? Um so so I think you know you can try to solve these tasks classically and you can try to build out all the physics models to make those realistic and then have them correlate. Um or you can try the datadriven route and make your video models better and like add rollout data into your video video model training so it gets more accustomed to failures and and how policies may not be optimal. Um and and I think there is a strong interest in that second route in the datadriven route and I'm sure that these models will get better. Um, and so maybe the main takeaway for the listeners from this discussion is not that these are always going to be bad. It's more like today if you want to have an evaluation that's a good proxy, probably doing something that's closer to a classic sim and maybe like a mixed approach like our hunts project um is going to give you a better result than just using a fully datadriven video model. But you know maybe a year from now or six months from now we can re-evaluate like this is one of the nice things here like we have this offline data set of real world performance and policies uh and initial states and so it's now very easy once a new better and awesome video model comes out we can just run that offline evaluation through the video model. Right now our needs to do a lot of grading when we have a lot of videos so that's a bit of a pain. Uh but if somebody figures out the grading aspect um then it will be a very easy test to run. Um yeah and I think there's a lot of potential in that it's just not yet today. >> I have a more uh I guess philosophical question from from this point which is that you know like in robotics we see so much evaluation we see like there so much benchmarks there so much you know people who do real evaluation to sim evaluation but there was I mean yes sure there will be points where there's a few benchmarks that people starts to converge towards then you set trace and from what I last check real tank is like at 98.5% in the last one one and a half years. So like what what's the thoughts here? Like is it the problem is like finding the right evaluation or is getting people on board on the evaluation? >> Like it's probably a bit of both, right? Like you need some agreement as a community what are good evaluations to compare on because if everybody makes their own and nobody compares then we don't learn much from an evaluation. Um, I think finding the right evaluation or finding the right tasks is also an important problem, right? Like as you said, we kind of a lot of people agreed on using Libero as their evaluation suite, but now it kind of like loses its discriminability. Uh, and kind of like everybody kind of does good on Libero now and it's a bit hard to say what is really going on. Um, and so I think agreeing on benchmarks is good, but also developing benchmarks that are either by design codeeveloping with the capabilities of the policies or that are easy enough to make or easy enough to extend that we can keep making them harder and harder as the policies get better. Um, I think is good. And I think one aspect specifically that I think benchmarks like Libaro don't push on too hard is generalization. Um, right. Like usually, at least so far, most of the benchmarks that people put out come with a training data set. Um, which is not something that happens in LM land anymore, right? Like when people make evolves in LLM land, they don't make training sets. They just make test sets and then you test whether the off-the-shelf model generalizes. Um, right? And and so in robotics, we haven't really done this because so far nothing generalized wide enough that this would make too much sense. Um, but I think one of the exciting aspects of this project is that you can actually kind of test zeroot generalization, right? like we can test it in environments we have never seen with objects we have never seen um and and the policies can do something and so now this is much more in a place where you can actually very quickly develop different kind of benchmarks that poke different capabilities right like in LM land people make benchmarks to test factuality and then benchmarks to test um you know I don't know uh language translation capability ability to do coding right and and so people very quickly come up with these different tasks and you build up this suite of benchmarks over time that allow you to holistically test what these models can do and and we haven't had that in robotics because the models didn't used to generalize at all. So we always kind of tested them on the train set. Um now I think we're getting to a point where it actually makes sense. And then tools like this tool uh I think are one way to get us to that same point where people can just propose oh like how does this model do in very cluttered scenes? Let me quickly make up a polarity val of very cluttered scenes. uh and now I can put it out, everybody can test it and we will know the next generation of models how well do they do in cluttered scenes right so there's a yeah I think there's a good point now where we can actually get to a mode like this um but we will still need buyin from the community like if nobody evaluates on it then there's not much point in making those benchmarks >> so like a following a question to this I think also maybe to clarify to the audience about the zero shop is that is the zero shop empowered by the fact of you a data set like Droid which is really you know great and beneficial for the community or is it more empowered by the base model which is pretty strong or you know like how do we resolve moving forward and you know get more people to be on board of this and you know get more models essentially to be to be you know running on this platform I guess that's >> yeah I think in my current feeling there's a lot of things to be done on this kind of benchmark by just using the droid data set um it usually helps to use a pre-trained model that like that makes your life a little bit easier because it trains faster and you know it's a decent initialization. Uh but I think there's a lot of interesting research to be done on the path from that kind of pre-trained checkpoint to the actual thing that you're going to evaluate. And like really so far all the models that we're testing here are models that are kind of like trained with a very vanilla kind of one pass of data filtering was done and then that's it. And then that's the model that we're putting out. Um and and so I think there's a lot of potential to iterate on this and I don't want people to think that it's all about who can collect the largest, you know, pre-training data set for their model and then that's how you win this kind of eval. Um but but I think there's a lot of interesting research to be done from that pre-train checkpoint to the actual thing that they're evaluating if that answers the question. >> Yeah. Yeah. I think that answers. Uh yeah. >> So uh what do you think what's next for this kind of benchmark then? you have you are you I guess you're trying you're putting this out there people are able to come up with their own polaris do you environment how hard is it to actually specify like a task like if I want to build a pyramid or something like this or put a peg in a hole like how to like how how do I how much work is that to add a new task in practice >> I think that I mean obviously it depends to some extent on the task but um the nice thing about like the rigid body task ask um is that like typically there's like a you don't have to do anything crazy to like I guess define a like a sparse reward or like success condition. >> Yeah. >> Um >> like I guess in this case we didn't really do any articulate object. Articulate objects are articulated objects are also like not too bad. So like for most pick and place tasks, it's usually like is this bounding box inside or overlapping to some extent with this other bounding box, right? I guess that's the case. I think the pyramid example that you gave is a little bit more unique. Um and that one would be a little bit harder because it's just like a chain of things. Um and I guess like lots of complex conditions. >> So to be clear, I think what Arhan is describing is defining the success condition. >> Yes. Right. There's a whole other part to like actually scanning objects and so on. >> Oh, sure. Yeah. >> And I would say that our pipeline here actually makes this relatively easy compared to if you had to kind of handcraft scenes and like handcraft objects and then do all of the other reward definition on top. >> Um yeah, I think I think we really tried to make all of this fairly easy. Um, so but basically we try to make it really easy for people to, you know, take either existing assets and puzzle them into new tasks, which I think is one very good way of making new evolves. Um, or scanning in their own assets. Like if you want to have a certain object in there or you want to have a certain scene in there, you can scan it. Um, you know, sometimes as easy as taking a few pictures with your phone or you can scan it with like some camera that you have on your robot. Um, then there's a gausian spotting pipeline that will make this like nice mesh for you. The mesh doesn't look very nice, but the visuals are much nicer. Um, and then there's this little tool that we built that's a browser tool. So, you don't even need to install anything. You kind of just drop all your assets in there. You can kind of puzzle them into the configuration that you like. And then you can click save and it will kind of save out all the assets and their initial conditions. Uh, and then that's basically the eval. And now you just need to define a reward like the success condition. I think Arhan has a little bit of code where you can just specify, okay, this object needs to be in this position and this object needs to be in this position. >> But that's already in EVA, right? And then you can kind of package that and you can upload it to Hagen face and now everybody can easily download and test it. >> Yeah. So this video on the right, I guess we're seeing you generating two objects or adding two objects, getting the scaling right, then you have this little green box, which is presumably the area where they're randomized, and I'm not quite sure how you're specifying the goal, but I'm assuming that's in there, too. So the goal is not in the GUI itself. The goal right now is being done via like some like I have like a very simple like you just paste like these condition like I have some functions defined for like X cube is on top or in this area or something. I think it's a little bit harder to do that graphically cuz at least to do like any task. Um, one thing that I wanted to also clarify is like you mentioned like it's not so easy that you can just scan it right now. So I think there's two aspects of this and it's just going to get easier as there's like progressions in like the 3D vision like world like for objects I would say you can easily just like take an image scan them in right now because it's like a generative model and as these other tools are developing like for example like these world labs uh like splat generations um I think it will become this is something I've been trying and I'm actually probably going to test like how evaluations hold up on this as Uh but seeing like just like taking an image and then like generating the splat for that and then evaluating it. Um like I guess at that point it would be something where you can just like draft up in like 5 minutes. Um yeah. >> Yeah. So so I think uh if we think about like what what should people take away from this or like what kind of users will this project have? I think there's like a few different modes. Um I think there's the mode of people who want to develop policies or just test capabilities of policies. Uh and so we released all the checkpoints that we fine- tuned with the certain data. Um so those work zero shot in new environments and we are you know hopeful at least that their performance will correlate uh with the real world. Uh and so you can download those checkpoints you can build a new evaluation scene and just run them and see okay how does pi 0.5 work in this kind of task or this kind of setting. Um and then there's the other user that actually wants to test um in one of their scenes. And so for them, we try to make it easy to scan in a new scene and then puzzle together their eval for their particular scene and use that for just policy iteration for them. Um, yeah, and then I think there's like kind of the maybe the in between user that just wants to use a bunch of existing assets and then puzzle them together into new scenes and then just upload it as an eval for people to use. Uh, and then that's also possible like all the assets that we created over the course of this project are available. uh and then you can scan in new ones pretty quickly. >> Have you thought about question? So I think like uh based on the understanding of this pipeline, it really makes it very easy to build a task with uh that's like more pick and place centric and I guess like one of the things that we talked about just now at the very beginning which is the kind of task and method to evaluation. I guess like for uh for like BA or like for like things that's been done in in pi you guys try out a bunch of tasks there's many tasks I guess like you know the the robot pick and everyone is like love to do close folding as one example which I think the point here is to show that you know classical approach or you know classical approach plus a bit of like VRM style will not be able to solve it but a pick and place task could be is actually solved with like a a engineer or temp kind of pipeline with like more of a VM estimation moving forward in this project. Are you guys going to move beyond just rigid body pick and place towards task that you know nonrehensile or like you know requires you to have more dynamics of interaction that no there's no way you can handcraft or handgineer a system essentially to try to tackle. >> Yeah, I think I mean so I guess I can comment on what my thoughts are on this. Um, like I like my my current view is like there's it's really hard to do like realto sim correlation for like these deformable type things. Um, like I guess like if you move if you like fold part of a shirt like how do you know if like it moved correctly? Um, and I guess the only really signal you get is like if you roll out whole trajectories with like similar initial conditions and like whether it succeeded or failed. Um, and at least from what I've seen with current simulators, like I think this will change like there's being a ton of work put into like simulation backends for like deformables and like particles. Um, this will improve. Uh but I think it also like going back to what we were talking about with video models, it's like unclear which one like to me right now it's unclear which one will improve faster. Like video models I guess would be more scalable but it depend like it's unclear like what data you need to make them work um and not like hallucinate and stuff like that. >> I see. It's like a race between building video models that's more physically informed versus building simulations that captures the rich uh you know dynamics of like uh deformable and sort of showing that correlation. So that's like two parallel track if I'm getting it. >> I think that's more or less like yeah what I'm envisioning. I don't know if anyone else has other thoughts. >> You know it's differentiable sim try to fit it to some small amount of data or something. I don't know. There's a huge spectrum I think but yeah >> yeah I think um you know I think we can also be a bit bit pragmatic here like I think this is a tool and I think it has a use uh which is currently limited to you know pick place and maybe articulate object tasks um but that's already a quite wide spectrum of tasks and it's probably also most of what at least a droid data set supports um and as long as we don't overfit the method to that particular type of task I think it can also be indicative of how well a method like this may work on a wider set of tasks. Um, right. And then I don't think our goal here to be clear is to replace real world evals. Right? Like I think exactly we want to eventually test on the full task distribution and that's best done in the real world. Um, but as a scalable tool for development, this serves a purpose, right? like similar to how people in autonomous driving evaluate a lot in simulation, but then they don't want to evaluate like the craziest snowy conditions uh with like eyes on the road in simulation because maybe their simulators aren't really accurate for that. Uh but you still get a lot of value and a lot of signal out of evaluating all the other cases that your simulator is sufficiently accurate for. Um, so you know, I I think I'm not sure whether like it's Arhan and my biggest desire to like become experts in particle physics simulators. Um, and figure out how to simulate the shirt to the highest degree of fidelity. Um, so you know, I think there's tools for different purposes and I feel that at least this version of the tool is most well purposeful for kind of pick place articulate object style tasks. Um, but then I'm sure we can use other tools to, you know, test on other tasks. >> Yeah. So, so like I I I'm mostly totally by that like 95% of things people want are probably pick and place tasks anyway. But like have you guys thought about doing like larger like you mentioned the world lab stuff. Do you think like there's a uh any room for like larger multi-rim environments? Do you think it would scale to like humanoid robots, mobile robots, stuff like that? Yeah, I think um like at least for right now like I I'm super interested to test this. The only I guess like I don't I haven't seen any like like I I feel like the policies aren't there like I guess like some some groups might have like internal policies like like Pi has their mobile robot stuff, Skilled has their mobile robot stuff, but there's not really like stuff that people can compare against each other. And so um the problem with that is it's like hard to know like if you can't like benchmark against other things then like you can get a number but you don't really know um what that number means. >> So so so the argument is this is there's no mobile robot foundation model so there's nothing to compare is that the >> I guess that's that's how I feel right now like I I think like otherwise >> open source mobile manipulation robots I guess there might be closed source models but there's no open source version out there. So, and so I guess >> I would argue that the 5.5 model is quite open source. Um, [laughter] maybe the robot runs on this. >> Yeah, I think uh yeah, but maybe the underlying question was whether the system would scale, right? Like can you do mobile robot evaluation and I think probably yes. Like I feel in some sense the hard part of SIM eval is not can you drive around and kind of find your right room and and stuff like this like driving has shown this for many years that you can do very successful sim eval for navigation. I think the hard part is kind of when you get really close and when it comes to interaction and like you know very very close uh visuals of what your simulation looks like. Uh and I think that part we have kind of tested in this project. And then you know having your robot drive from one part of the kitchen to another seems kind of like an easier part to simulate. Um it's possible that as your task horizon grows kind of the accumulation of small arrows will get worse. Um, so maybe you know our current tasks are like 30 seconds or less. Uh, and so maybe if your task takes 10 minutes, then small differences in your simulator can really hit you somewhere in that 10-minute task. And so then maybe correlations get worse. Again, I think I could see that. Uh, but just in terms of like scanning large scenes or like having robots drive around in large scenes and sim, I don't feel that's a kind of core technical problem. >> Guess website wise. I guess I don't know if you guys have seen the eval videos, but this is kind of what they look like. Um, comparing uh real versus like our sim evaluations. One of the downsides of this of like using these gouch spotting things um potentially this might be like uh improved by people in like the vision world, but like you kind of lose your shadows. Um it's hard to like the shadows onto splats. Um, people have done some hacks around this, but it's like I guess there's not like one really clean way to do it. Uh, but yeah. >> Do you see that making a big difference though? I mean, obviously, no. But >> I think so early in the project, we were having like a bunch of issues on like visual shift. Um, but this is like there were so many more issues than just shadows. there was like controller issues and like uh just like other like the one thing is like a bunch of policies are very over like to wrist camera so if there's any shift there it's like going to mess up your policy. Um, I think now what I think is it it'll improve a little bit, but there's just like so many small things in like reconstructing your simulation that's going to be different. Like even if shadows are there, like your lighting is probably going to be different. Your color grading is a big thing. Um, and going to simulation, like how do you color grade same as like your real world camera? Um, so it might be a small improvement, but I think with just like so many different things that are hard to match exactly, um, just adding shadows is not going to make the delta that you might want or expect. >> So like lighting and texture quality, things that are under your control, basically those those are those are more important than the than the shadows. Is that >> Yeah, I'd say they're under your control, but matching them is kind of like it's kind of unclear how to match them. Um but yeah, I think >> [snorts] >> uh if you are able to match them, then that'll probably do make an it'll be an easier delta to make. Um >> what's the hard part of matching them? So like you have the gshion spotting scans that in. So is that I guess it sounds like that process is mostly automated and then there's the object generation which is generative AI and so the objects do look maybe a little bit less realistic than your backgrounds arguably. But is that the is that about a reasonable summary or what do you >> So okay it kind of depends on like so okay if you ray trace the objects versus like uh splat them I guess like the splatted objects if you're if you're scanning with the same camera that you're testing with your the lighting things will have a smaller effect given that your like splatting pipeline is able to like faithfully reconstruct it uh to good degree. Um, but then if you start like mixing ray trace objects in with it, uh, and especially if they're like generated, uh, you will kind of lose that faithfulness. Uh, I think did I don't know if I missed part of the question. >> Yeah. >> Yes. >> If you look for example in this scene, if you look at this like green tablet that that's on the table there, right? Like in the real world, it has these specular effects of the overhead lighting and it's just really really hard to get a simulation to be faithful to that without a lot more effort on trying to reconstruct the lighting in the scene and actually giving the right material and then using the right level of fidelity in your rate tracing to get that accurate. Um but then you know maybe one of the takeaways here is that our goal isn't actually perfect accuracy. Our goal is to be just like kind of barely good enough so that your that your correlation holds. uh you don't need to get like the perfect looking simulator. >> Yeah. So, so I guess so the takeaway here then would be that like there are obvious differences between the left and the right ear, but like the shadows and specularities, the reflectivity of the table, all that, but that is not doesn't seem to matter, I guess, or at least you can get good enough correlations to to get a good signal >> at least with some sim right like this is the caveat here. Like if you do it fully zero shot, you know, it may actually matter. Uh but maybe once you tell the model that the specular details don't matter or that like the lack of shadows isn't the most important part of the scene then it can do much better. >> Excellent. I think one of the interesting I observe I was looking at the website is that the control sort of capture some of these lighting and shadow >> uh >> in the generation. >> Yeah. >> So that doesn't help right because there's a lot of artifacts in the way he generates. >> Well it gets those but then it's got the objects are changing shape as he moves right. Yeah, I think it's like one of the benefits that come with like the learning from data is like it captures like how things might be reflective and how like if you move the light doesn't necessarily move in the object frame. Um, and yeah, I like one also thought is that like as the actual like base policies or like the foundation models like the robot policies just get better, um, hopefully they become more invariant to all these changes, whether it's in world models or in like these like reconstructed environments. Um, and so it might just be like you can do all this tuning for your simulator to just like get your vibe checks um to do policy iteration, but it might not actually matter as like the models just improve. >> I guess I guess that was the follow was like I think that at least my experience has always been like the specularities and shadows are the first things that the domain that domain randomization teaches it to ignore. So it seems like that'll just get handled by having more data in your training mix, right? I buy that. That makes sense. >> Yeah, it it is true. But I think there's like some subtleties that aren't obvious to humans that may throw these policies off. I think this is one of the learnings of this project basically like >> very quickly Arhan had environments that to a human looked pretty good right and we were like oh this looks actually great like awesome let's test some policies but then actually getting them to correlate is much harder um and and the policies looked visually much worse in sim in their behavior. Um, and it's just that, you know, small differences in the statistics of these images between real and sim can really throw off models that have only ever seen real world statistics. >> Um, yeah. >> Excellent. >> Yeah. Maybe one other uh kind of good thing to mention is that we so the kind of the setup and the robot and the control code here very closely mirrors what we used in Robo Arena. Um, which is kind of a real world version of this benchmark. Um, which I think we talked about in this podcast. Yeah, we did a podcast on this. It was uh like 6 months ago. So, scroll back uh you will see the real world version. Um and and so the nice thing is a we tested that actually the correlation is kind of surprisingly well hold with Robo Arena. Uh I don't want to like put too much emphasis on this result because it it seems very sketchy to me. Um I think it's kind of a somewhat happy coincidence that they worked out so well. But I think you know at least there is some correlation. I will be happy to claim that. Um, and the code is the same in some sense. So like once your policy works in the sim environments, it's like a one-click change to then say, okay, let's like run it on the real robot and and run it through robot arena. Um, so the hope is basically that people will use this as a tool to very quickly iterate uh on policies uh that they can eventually then submit to the real deal uh real world eval uh and actually show people that it works on a much broader set of tasks as well. That's maybe one way to also guard against the overoptimization of these sim benchmarks is to just say well you know it's very easy to then actually go to the real world equivalent and make sure it also works in real. >> What makes you think this is sketchy this part? Like what what are your what are your concerns? >> You know like there's like a nearly one like the correlation pearson correlation is nearly one like 98. >> It looks too nice. It came out too well. Okay. All right. >> Yeah. And you know like the task distribution is somewhat different right like we're evaluating really only on pick place tasks here. It is diverse environments but it's p place and in robot arena there are some you know soft object manipulations and so on like some wiping and some picking up towels and stuff like this. Um >> that's a different robot too because you're doing droid, right? So >> So robot arena is also droid. So the the robot is the same, the control code is the same, the training data set is the same. >> Um but the task distribution in my mind is a bit different. Um and the correlation here is higher than the correlation between Arhan's extremely controlled paired experiments and our semi valves. So that seems a little too good to be true. Um so I don't want to like you know put my hand to fire that like 0 98 correlation is what you will get for everything. uh but but I think there is clearly a sign that the evaluations do correlate to some extent with the real. Do you guys have other philosophical questions? >> I guess maybe like just one take away from today's uh sharing is like how do you want people to sort of use this work and this tool and how you know at the end of the day what's the long-term vision? Okay, since you use this tool, you should probably go use robot arena like maybe you can summarize the the bigger picture here that you like you know moving forward. Yeah. >> Yeah. I think like like one of the focus that we did is making it as you see as easy to use as possible and so like and like uploading environments. Um, so the hope is like we can like right now we put on everything that we we put out everything we tested on um between like Udub and Princeton. But like I guess as benefit of being able to like cheaply scan it and not have to handcraft is like you can actually crowdsource these kind of things by just like taking videos. Um and so we hope that people will we have this like polaris hub on hugging face um where you can pull our environments but also once you made your own environment uh we actually have a script in our repo that literally just like uploads your environment as a PR to our like Polaris hub. So hopefully we can kind of gather like a suite of these tasks like across universities across institutions um like different locations. Um, and I guess the downstream goal is like what this is like what I think like it'd be useful for is just like doing policy iteration um more meaningfully and with like more diverse tasks that you can like if people have these other tasks that you're not necessarily thinking about um you also get to evaluate on them if you're evaluating on the entire like polar sub um and so I think it'll just um policy evaluation for the sake of policy development much easier and then like in the end you and evaluate on the real world tasks that you care about um once you've like seen like okay this policy has been getting better on this um sim evaluation but I think that's the overall goal >> yeah and I think there's like really two kinds of users I think I mentioned this earlier like basically the people who just want the benchmark and so I think we have some environments as Arhan mentioned I think we'll try to make a few more that we think are good and verified um and we'll just put those out so people have kind of a benchmark and if they just want to download something existing and they want to evaluate on something other than libero then that's a thing they can do now um but then I yeah really hope that there will be people who make new benchmarks with this and who just like you know basically leverage the power of the community to really crowdsource how to evaluate these policies so that when somebody puts out a new policy and wants to claim that it's the best and baddest policy that has been um then they have to evaluate it on all of these different benchmarks and kind of very holistically get a picture of how good this policy is in different in different uh scenarios and I think we have seen this work really well for well at least to some extent really well for LLMs and vision models um where there is a very large community of people who build different benchmarks and then share them and people compare um and we haven't seen it so much in robotics yet but I think with tools like this it is actually feasible and I hope that we get there eventually >> uh yeah on that I guess if if you haven't implemented a simul if you're listening to the podcast or something and haven't implemented a simulation environment This is like a ton of work. So, it's definitely great to have more tools that make it easier like this, I think. Um, I I wanted to ask like one last follow-up question, like how do you think about different robots? How easy is it to add like there's the Yam Arms or Aloha or whatever else? Like cuz you're I think all your stuff here was the droid, right? you guys going to support everything or >> so I guess like uh I mean I think the the model itself for the robot has to be crafted and I think this is okay because it's usually a onetime cost unless you're like making a ton of different embodiment with all different features. Um so as long as you have this like even if it's like a Majoka URDF model like um I guess for the visual aspect of it it's like similar just a scan and it becomes like automatically articulated. Um but at the end of the day I think you will still need to build a model of the arms or like mobile platform that you're using. >> Yeah to be honest I don't think the main blocker to adding more robots is the part of adding more robots into the sim. uh it's having data in the open that allows people to actually test on different robots. Uh like to my knowledge, Droid is really the only fully supported open source platform that has both the robot and a sufficiently diverse data set out there. I think Galaxy to be fair has a fairly diverse data set too now. So I think that's the only other one that I know of um where there is sufficient data diversity in open source that people can train on it and then test in unseen tasks and environments. >> Yeah. Um, so yeah, I think if those data sets exist for the Yam arm or like any other robot, then it wouldn't be too hard to add it to the evaluation, but it only makes sense once those data sets exist. >> Great. Uh, well, any last uh questions, Jeff? Anything >> or otherwise? >> No, I think I'm good. >> Awesome. Well, really cool work. One last thing that we always like to ask everybody, do would you guys like to give out a give a shout out to any other papers or any cool stuff that you've seen recently? >> I guess like a lot of the stuff that or like some takeaways and the stuff we're building on was like based off of Simpl's results. So I guess shout out to Simpler. >> Shout out. >> We also need to shout out all the 3D gausian splatting and and vision people who are helping us a lot here. I think they they're developing awesome stuff. I think you know while we were developing this project there were a lot more papers coming out on like you know trellis 2 I think the week of our release the marble model the week before our release I think. So there's like and then Sam 3 also came out around that time or Sam 3D is kind of the most relevant part here. So I think there's like a lot of awesome development there and I think people are really excited. Uh, I think we need to like stay somewhat measured here and like actually test do these things correlate once you once you use them to build your sim, right? Like it's much easier to make something that looks vaguely nice to a human versus something that actually is useful as a evaluation tool. But I think there's a lot of development and I'm sure that these things will only get better. So I think that's quite exciting. >> There's always so much cool work coming cool stuff going on in robotics these days. Well, thank you guys for coming on. It was great. Uh, great stuff as always. Uh, yeah. >> Yeah. Thanks,